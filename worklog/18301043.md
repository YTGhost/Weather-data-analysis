# 工作日志

## Day1

spark与hadoop下载并设置环境变量

pip install导入四个包

vmware安装

熟悉git与github的使用方法，拉取远程仓库奖励好的项目

修改readme.md文件并push，合并到测试分支dev

## Day2

学习时间序列相关算法和模型(by bilibili)

学习numpy使用方法

pandas包查询使用方法

使用git提交日历，熟练使用git

使用1980-2014年的34年北京的6月28日最大气温数据，尝试构造ARIMA模型，
并成功得出一阶差分和二阶差分图像，确定采用一阶差分，模型以一个参数参数d=1

## Day3

今天上午，在Day2的基础上，计算模型需要的p和q

尝试各种方法，终于画出了acf与pacf

根据图片判断p和q的值，在建模的时候，模型训练完一预测就报十几个错，无法预测

然后，改了一上午写法，终于放弃，打算改变策略，换一种方法，新建个工程重新算

下午在查询了很多很多很多资料之后，发现p和q的计算方法有很多，不过暂时只用了看图的只管方法，打算明天完善一下

用新方法计算d、p、q后进行平稳检验、白噪声检验通过之后，继续构造模型，还是报错，在Google之后，发现是没有给DataFrame加时间周期导致的

然后，加上时间周期后，终于可以在控制台打印预测的未来七年的数据了(因为原文件给的是1980-2012年的32年的6月28日的气温情况，只有32组数据，所以时间周期为一年)

然后，又查资料，把得到预测结果这个数据类型转换成DataFrame，更改列名，写入.csv文件

大功告成后，把工程加到小组项目里，git请求合并分支。

虽然我的模型ARIMA能预测了，但是每次都需要手动使用工程里被注释掉的，用来计算d、p、q三个参数的部分，然后再预测，所以还有改进空间，争取明天改成自动化的。

请求合并后有些空余时间，看了一下SNN循环神经网络，据说可以改进预测模型....看了看，好难

## Day4

今天上午尝试使用深度学习建模，完善ARIMA模型

查资料后决定使用LSTM

再我在Google找到教程，一步一步写代码的时候发现需要装sklearn

在装完sklearn后发现需要keras

在安装keras的时候发现需要tensorflow

正在我准备安装tensorflow的时候发现它必须使用python3.6以及以下的版本

于是开始安装anaconda

下午开始改进昨天完成的ARIMA模型，因为昨天的ARIMA之中，三个参数必须手动做比较，才能得出。

最后，可以将三个参数在程序内自动化计算出，并传给模型进行训练了，也算一大突破。

然后，昨天的模型只对测试文件中的最大温度进行了预测，今天又加上了最小值和平均值的预测，将三者一起写道csv文件里。

总的来说，今天上午尝试了深度学习来优化模型，下午对ARIMA模型进行了改进与完善。

## Day5

今天将前几天测试模型用的按年为周期的1980-2012年的32个数据的测试文件，改成了正式的预测文件，也就是有一万四千多北京数据的文件。

由于每天的数据，最大温度和最低温度，在2012年之后有很多天数的数据是丢失的，而只有平均温度都没有丢失，所以只预测了未来七天的平均温度。

昨天写的模型的文件，默认传入的文件中包含最大温度、最小温度和平均温度，但是今天发现最大、最小温度不全，所以正式的预测文件只有平均温度，所以需要更改模型。

考虑到日后可能会对最大、最小温度预测，所以并没有直接更改原模型，而是重新新建了一个文件夹，对1980-2020年，1w4的数据进行预测，并放到csv文件中。

在使用正式文件时，更改了之前模型中的按年为周期，改为按日为周期。

在模型的计算参数p和q的过程中，由于数据量过大，模型非常慢，所以我限制了p和q的取值，为0-8之间的数字，在8*8个取值对中找到最优参数对。

根据预测模型，画出了预测图，成功将预测数据导出。