# 工作日志

## Day1

spark与hadoop下载并设置环境变量

pip install导入四个包

vmware安装

熟悉git与github的使用方法，拉取远程仓库奖励好的项目

修改readme.md文件并push，合并到测试分支dev

## Day2

学习时间序列相关算法和模型(by bilibili)

学习numpy使用方法

pandas包查询使用方法

使用git提交日历，熟练使用git

使用1980-2014年的34年北京的6月28日最大气温数据，尝试构造ARIMA模型，
并成功得出一阶差分和二阶差分图像，确定采用一阶差分，模型以一个参数参数d=1

## Day3

今天上午，在Day2的基础上，计算模型需要的p和q

尝试各种方法，终于画出了acf与pacf

根据图片判断p和q的值，在建模的时候，模型训练完一预测就报十几个错，无法预测

然后，改了一上午写法，终于放弃，打算改变策略，换一种方法，新建个工程重新算

下午在查询了很多很多很多资料之后，发现p和q的计算方法有很多，不过暂时只用了看图的只管方法，打算明天完善一下

用新方法计算d、p、q后进行平稳检验、白噪声检验通过之后，继续构造模型，还是报错，在Google之后，发现是没有给DataFrame加时间周期导致的

然后，加上时间周期后，终于可以在控制台打印预测的未来七年的数据了(因为原文件给的是1980-2012年的32年的6月28日的气温情况，只有32组数据，所以时间周期为一年)

然后，又查资料，把得到预测结果这个数据类型转换成DataFrame，更改列名，写入.csv文件

大功告成后，把工程加到小组项目里，git请求合并分支。

虽然我的模型ARIMA能预测了，但是每次都需要手动使用工程里被注释掉的，用来计算d、p、q三个参数的部分，然后再预测，所以还有改进空间，争取明天改成自动化的。

请求合并后有些空余时间，看了一下SNN循环神经网络，据说可以改进预测模型....看了看，好难

## Day4

今天上午尝试使用深度学习建模，完善ARIMA模型

查资料后决定使用LSTM

再我在Google找到教程，一步一步写代码的时候发现需要装sklearn

在装完sklearn后发现需要keras

在安装keras的时候发现需要tensorflow

正在我准备安装tensorflow的时候发现它必须使用python3.6以及以下的版本

于是开始安装anaconda

下午开始改进昨天完成的ARIMA模型，因为昨天的ARIMA之中，三个参数必须手动做比较，才能得出。

最后，可以将三个参数在程序内自动化计算出，并传给模型进行训练了，也算一大突破。

然后，昨天的模型只对测试文件中的最大温度进行了预测，今天又加上了最小值和平均值的预测，将三者一起写道csv文件里。

总的来说，今天上午尝试了深度学习来优化模型，下午对ARIMA模型进行了改进与完善。

## Day5

今天将前几天测试模型用的按年为周期的1980-2012年的32个数据的测试文件，改成了正式的预测文件，也就是有一万四千多北京数据的文件。

由于每天的数据，最大温度和最低温度，在2012年之后有很多天数的数据是丢失的，而只有平均温度都没有丢失，所以只预测了未来七天的平均温度。

昨天写的模型的文件，默认传入的文件中包含最大温度、最小温度和平均温度，但是今天发现最大、最小温度不全，所以正式的预测文件只有平均温度，所以需要更改模型。

考虑到日后可能会对最大、最小温度预测，所以并没有直接更改原模型，而是重新新建了一个文件夹，对1980-2020年，1w4的数据进行预测，并放到csv文件中。

在使用正式文件时，更改了之前模型中的按年为周期，改为按日为周期。

在模型的计算参数p和q的过程中，由于数据量过大，模型非常慢，所以我限制了p和q的取值，为0-8之间的数字，在8*8个取值对中找到最优参数对。

根据预测模型，画出了预测图，成功将预测数据导出。

## Day6

今天主要是使用ARIMA模型将全国各省会的数据预测了出来，花了不少时间。

基于曾经学过的html,css,js,下一周打算开启前端的开发。

然后开始看Vue框架的学习视频，复习了v-bind, v-for, v-model等等指令。

在个人电脑上建立了一个本地Vue小项目项目，也就是对今天学习内容的一个训练实践。

打算今天晚上将Vue视频完整看完，然后开启支线任务的前后端开发。

## Day7

今天利用学习的html css的知识，写了一个支线任务用户管理系统的登录和注册的html页面。

使用<script></script>标签引入vue，使用v-model和v-on指令，实现页面跳转

在完成以上两个.html文件之后，感觉到页面布局美化的吃力，于是在B站看了看一个css+dev的布局的小视频，并做了一个小demo

我发现，尽管我学过Vue，但是我看过的教学视频都是在html文件中直接引入vue，而不是使用vue compoment，而工程化的使用vue框架，需要更深一步的学习。

有一说一，在模型建立完成，跑完数据之后，感觉自己不知道该干啥了...虽然自学过一部分ue框架，但是并没有工程化地去使用。

用户管理系统的html登录、注册页面我今天写出来了，但是不知道能不能直接在项目里直接用这两个页面，还有就是，在放到云服务器上时，不知道如何调用后端接口。

所以，今后几天我的学习中心将转移到Vue框架的工程化的学习，以及对后端接口的调用的知识。

## Day8

今天上午，将Vue官方文档浏览了一遍，大致了解了Vue框架的核心——组件是如何建立与使用的。

然后，由于昨天写出来的登录和注册的页面是html页面，而不是以组件形式存在的，我需要将登录的小框和注册的小框以组建的形式写入工程。

所以，今天下午就对登录和注册页面进行了改动，将他们.html文件改成了.vue组件，并加入到前端工程的compoment文件夹下。

在学习如何运行的时候走了不少弯路，最终知道了运行方式，即：1、命令行导入工程下2、npm i	3、npm run serve，就可以打开页面进行查看了。

经过实践，我改动的登录和注册的组件是可以放到工程页面上去的。

不足的是，我并不太了解前端如何对接后端，虽然直到需要给后端传递request，但是并不知道如何操作。于是就只做了页面布局和美化组件的工作。希望之后的几天可以学习到如何和后端对接的办法。

今天才知道的是，最终答辩实践不是周五，而是周日或下周一，有充足时间研究模型，所以我开始打算写完这两个页面之后开始重新投入到模型优化上去。

在晚上的时候，我开始就之前对LSTM踩过的坑开始决定，学习使用anaconda，并创造并使用conda环境。

学习一些conda指令之后，在不断查资料后，成功建立了conda虚拟环境，成功加载tensorflow与各包，并在pycharm项目中采用了创建的conda环境。为明天重启的LSTM做好准备工作。

## Day9

今天开始正式重新拾起上星期被放弃的LSTM模型。在使用conda虚拟环境建立的pycharm项目下进行写模型。

在我从各个教程上学习的时候，发现是大部分的LSTM博客都是对数据集一分为二，一者为模型构造部分，二者为模型测试部分。经过按步骤编码后，发现LSTM模型拟合效果还是很不错的。

然后，就根据教程，实现了使用LSTM模型，预测未来一个数据。经过大量的查找，我发现大多数都是只有预测一个值的功能。在对test.csv预测，发现可以正常预测后，决定开始改变模型，从只能预测一天到七天。

在经过思考后，我决定采取滚动预测的方法，也就是预测出1值，再用1预测2，然后再用2预测3等等。这样，就可以将只预测未来一个数据的模型，转换成预测多个数据的模型了。

经过模型输出，我发现，当设置迭代300次以上时，mse值可以降低到1以下。但是，在基数特别大，也就是正式数据14000个数据，此时，一次迭代的时间就30s左右了，迭代300次时间还是很长的。

所以，我打算在今天晚上和明天，使用今天写好的模型，对全国的数据进行重新预测。由于数据量大，跑模型时间还是很长的。